# LSF-AI-experiment
Experiments for a Personal Assistant (like S*ri, Cort*na, A*exa) for deaf people by using French-Sign Language recognition. (WORK IN PROGRESS)

10/01/2021
This was supposed to be that.
But in fact... 

Here you can find the workprogress of my "personnal assistant" made with p5js & ml5js. 
The first idea was to create a "gesture assistant" to allow deaf people to use it, by using French-Sign Language recognition.
But, due to the actual state of hand recognition stuffs, and the clearly not satisfying results with teachable machine, I created a new body language, 
which can be understand by poseNet. Thanks to @theCodingTrain, (Dan Shiffmann the best), i implemented ml5 and trained a NeuralNetwork to recognise 
some positions in space. WAY LESS SERIOUS HAHA I let you see.

Which lead me to think that this virtual assistant is not only for deaf people anymore, but can be experimented by people with no physical disabilities. It opens a
new way of seeing our technology and who are the main target of each ones. 

So Here I propose a new way of interacting with personnal Assistant, with gesture recognition. 
Maybe, to touch more and more people with these tech, can we mix vocal and gesture recognition inside our personnal assistant ? 
(And maybe begin to think about personnal assistant more as useful tools for "not entirely valid" people, more than rich valid people ...? Hum... Is it beginning to be political ?)


Here you can find my step by step workflow, not very clearly organised for the moment.
You have codes, pictures, screenshots,...

Let's try to make the world a better place. (and funniest pliz)
